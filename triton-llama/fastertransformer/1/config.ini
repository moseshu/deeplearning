[llama]
model_name = llama_7b
head_num = 32
size_per_head = 128
inter_size = 11008
num_layer = 32
rotary_embedding = 128
layernorm_eps = 1e-06
vocab_size = 49954
start_id = 1
end_id = 2
weight_data_type = fp16
