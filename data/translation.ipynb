{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d005512-6c00-4be4-a1bb-a5ffdd9f83de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt19 (/DATA/jupyter/personal/translation/./data/wmt19/zh-en/1.0.0/29e210fae5690e843cae5dc43b53db36c4e02f927db50cd5235a22ab42dde90a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946f664fb8804bee94c0592c52430797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "lang_pair='zh-en'\n",
    "data = datasets.load_dataset(\"./data/datasets/datasets/wmt19\",name=lang_pair,cache_dir=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad50e69-8760-4856-ab04-e036623e361d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3943ac66-8068-455c-a344-bc167525d81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18094"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alt_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc0a457d-a289-4097-9343-d64ed58ecf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer,tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "import json\n",
    "import torch\n",
    "import jieba\n",
    "train = data['train']\n",
    "validation = data['validation']\n",
    "for i,item in enumerate(train):\n",
    "    print(item['translation'])\n",
    "    if i == 3:\n",
    "        break\n",
    "eng_data = []\n",
    "zh_data = []\n",
    "for i,item in enumerate(train):\n",
    "    english = item['translation']['en']\n",
    "    english = \" \".join(nltk.word_tokenize(english.lower()))\n",
    "    eng_data.append(english)\n",
    "    chinese = item['translation']['zh']\n",
    "    chinese = \" \".join(jieba.cut(str(chinese.replace(\" \",\"\")), cut_all=False, HMM=True))\n",
    "    zh_data.append(chinese)\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a473d3b-095c-4f8a-8dfe-4181fe3d01c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "val_eng_data=[]\n",
    "val_zh_data=[]\n",
    "for i,item in enumerate(validation):\n",
    "    english = item['translation']['en']\n",
    "    english = \" \".join(nltk.word_tokenize(english.lower()))\n",
    "    val_eng_data.append(english)\n",
    "    val_chinese = item['translation']['zh']\n",
    "    val_chinese = \" \".join(jieba.cut(str(val_chinese.replace(\" \",\"\")), cut_all=False, HMM=True))\n",
    "    val_zh_data.append(val_chinese)\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7034fbf-482a-45d2-aba9-265d732c7a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_data_js = []\n",
    "zh_data_js = []\n",
    "import json\n",
    "with open(\"./data/en-zh/translation2019zh_train.json\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "for data in lines:\n",
    "    json_data = json.loads(data)\n",
    "    english = json_data['english']\n",
    "    english = \" \".join(nltk.word_tokenize(english.lower()))\n",
    "    eng_data_js.append(english)\n",
    "    chinese = json_data['chinese']\n",
    "    chinese = \" \".join(jieba.cut(str(chinese.replace(\" \",\"\")), cut_all=False, HMM=True))\n",
    "    zh_data_js.append(chinese)\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f0ae358-33b1-47d6-b7e8-7f5996400fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1929 年 还 是 1989 年 ?',\n",
       " '巴 黎 - 随 着 经 济 危 机 不 断 加 深 和 蔓 延 ， 整 个 世 界 一 直 在 寻 找 历 史 上 的 类 似 事 件 希 望 有 助 于 我 们 了 解 目 前 正 在 发 生 的 情 况 。',\n",
       " '一 开 始 ， 很 多 人 把 这 次 危 机 比 作 1982 年 或 1973 年 所 发 生 的 情 况 ， 这 样 得 类 比 是 令 人 宽 心 的 ， 因 为 这 两 段 时 期 意 味 着 典 型 的 周 期 性 衰 退 。']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_word = []\n",
    "for i in zh_data:\n",
    "    field = i.split(\" \")\n",
    "    \n",
    "    tmp = []\n",
    "    for j in field:\n",
    "        if text_utils(j,'zh'):\n",
    "            if len(j) != 1:\n",
    "                x = [k for k in j]\n",
    "                tmp.extend(x)\n",
    "            else:\n",
    "                tmp.append(j)\n",
    "        else:\n",
    "            tmp.append(j)\n",
    "    \n",
    "    zh_word.append(\" \".join(tmp))\n",
    "zh_word[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beb5b141-992b-4163-b1fd-8d33612a1c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1929 年 还是 1989 年 ?',\n",
       " '巴黎 - 随着 经济危机 不断 加深 和 蔓延 ， 整个 世界 一直 在 寻找 历史 上 的 类似 事件 希望 有助于 我们 了解 目前 正在 发生 的 情况 。',\n",
       " '一 开始 ， 很多 人 把 这次 危机 比作 1982 年 或 1973 年 所 发生 的 情况 ， 这样 得 类比 是 令人 宽心 的 ， 因为 这 两段 时期 意味着 典型 的 周期性 衰退 。']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf80645f-8144-4a25-a293-c44922e35435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25984574\n"
     ]
    }
   ],
   "source": [
    "print(len(eng_data))\n",
    "with open(\"./data/en-zh/wmt19.zh\",\"a+\") as f:\n",
    "    for i in zh_word:\n",
    "        f.write(i+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3cad5b-0d2c-4d2f-9e75-7a4788c35ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_translation_data(translations, tokenizer, seq_len=128):\n",
    "  \n",
    "    input_text = translations['en']\n",
    "    target_text = translations['zh']\n",
    "\n",
    "    if input_text is None or target_text is None:\n",
    "        return None\n",
    "\n",
    "    input_token_ids = encode_input_str(\n",
    "        input_text, target_lang, tokenizer, seq_len, lang_token_map)\n",
    "  \n",
    "    target_token_ids = encode_target_str(\n",
    "        target_text, tokenizer, seq_len, lang_token_map)\n",
    "\n",
    "    return input_token_ids, target_token_ids\n",
    "\n",
    "def transform_batch(batch, lang_token_map, tokenizer):\n",
    "    \n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for translation_set in batch['translation']:\n",
    "        formatted_data = format_translation_data(\n",
    "             translation_set, tokenizer, max_seq_len)\n",
    "    \n",
    "    if formatted_data is None:\n",
    "        continue\n",
    "    \n",
    "    input_ids, target_ids = formatted_data\n",
    "    inputs.append(input_ids.unsqueeze(0))\n",
    "    targets.append(target_ids.unsqueeze(0))\n",
    "    \n",
    "    batch_input_ids = torch.cat(inputs).cuda()\n",
    "    batch_target_ids = torch.cat(targets).cuda()\n",
    "\n",
    "    return batch_input_ids, batch_target_ids\n",
    "\n",
    "def get_data_generator(dataset, lang_token_map, tokenizer, batch_size=32):\n",
    "    dataset = dataset.shuffle()\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        raw_batch = dataset[i:i+batch_size]\n",
    "        yield transform_batch(raw_batch, lang_token_map, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57e35e5e-bb51-40ac-91c7-b1ee54b10343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def load_txt(file_path):\n",
    "    data = []\n",
    "    with open(file_path) as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            line = line.replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "            data.append(line)\n",
    "            line = f.readline()\n",
    "    return data\n",
    "lines = load_txt(\"./data/tatoeba/train.zh\")\n",
    "\n",
    "lines = [l.replace(\" \",\"\").replace(\"-\",\"\").replace(\"\",\"\").replace(\"и\",\"\") for l in lines]\n",
    "\n",
    "data = [\" \".join(jieba.cut(str(s), cut_all=False, HMM=True)) for s in lines]\n",
    "with open(\"./data/tatoeba/dev.split.zh\",\"a+\") as f:\n",
    "    for i in data:\n",
    "        f.write(i+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27919a3f-148a-45a6-ad5f-88e4067eef4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end... \n",
      "end... \n",
      "中文句子数量：7194,日文句子数量:7194\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "\n",
    "def handel_data(input_file,out_file,lang='zh'):\n",
    "    count = 0\n",
    "    size = 4\n",
    "    \n",
    "    with open(input_file) as f:\n",
    "        tmp = list()\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            count += 1\n",
    "            line = line.replace(\" \",\"\").replace(\"-\",\"\")\n",
    "            if lang == 'zh':\n",
    "                data = \" \".join(jieba.cut(str(line), cut_all=False, HMM=True))\n",
    "            elif lang == 'ja':\n",
    "                data = \" \".join([m.surface() for m in tokenizer_obj.tokenize(line, mode)])\n",
    "            tmp.append(data)\n",
    "            if len(tmp) == size:\n",
    "                # print(\"write\",len(tmp))\n",
    "                with open(out_file,\"a+\") as fp:\n",
    "                    fp.writelines(tmp)\n",
    "                tmp.clear()\n",
    "            line = f.readline()\n",
    "        \n",
    "        \n",
    "        if len(tmp) > 0:\n",
    "            print(\"end... \")\n",
    "            with open(out_file,\"a+\") as fp:\n",
    "                fp.writelines(tmp)\n",
    "        \n",
    "    \n",
    "    return count\n",
    "\n",
    "number_zh = handel_data(\"./data/ja-zh/test.dedup.zh\",\"./data/finall/test.zh\")\n",
    "number_ja = handel_data(\"./data/ja-zh/test.dedup.ja\",\"./data/finall/test.ja\",'ja')\n",
    "print(\"中文句子数量：{},日文句子数量:{}\".format(str(number_zh),str(number_ja)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffc57e33-7e02-4239-81a9-a7ff07e1c37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def load_txt(file_path):\n",
    "    with open(file_path) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    return lines\n",
    "\n",
    "def write_txt(data:list,file_name):\n",
    "    with open(file_name,\"a+\") as f:\n",
    "        for i in data:\n",
    "            f.write(i.replace(\"\\n\",\"\")+\"\\n\")\n",
    "        \n",
    "from opencc import OpenCC\n",
    "\n",
    "zho = load_txt(\"./data/tatoeba/dev.zh\")\n",
    "jianti = [OpenCC(\"t2s\").convert(s) for s in zho]\n",
    "write_txt(jianti,\"./data/tatoeba/dev.jianti.zh\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c6ab779-a419-4e2d-b916-14d904e2625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "\n",
    "tokenizer_obj = dictionary.Dictionary().create()\n",
    "with open(\"./data/tatoeba/test.ja\")  as f: \n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "lines = [l.replace(\" \",\"\").replace(\"-\",\"\") for l in lines]\n",
    "mode = tokenizer.Tokenizer.SplitMode.B\n",
    "\n",
    "data = [\" \".join([m.surface() for m in tokenizer_obj.tokenize(i, mode)]) for i in lines]\n",
    "with open(\"./data/tatoeba/test.split.ja\",\"a+\") as f:\n",
    "    for i in data:\n",
    "        f.write(i+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6702f2e-59ff-4cc0-848e-168dd75ed92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21fc402a-d106-4a25-bde9-cd0203bcb964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sentencepiece as spm\n",
    "# op = \"--input=./data/finall/test.zh,./data/finall/test.ja,./data/finall/train.ja,./data/finall/train.zh --model_prefix=./sentpiece.bpe --vocab_size=15680 --character_coverage=1.0 --model_type=bpe\"\n",
    "# spm.SentencePieceTrainer.Train(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a1d534c3-ef44-457e-ac6c-e71dc2b0ede4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from pysubs2 import SSAFile, SSAEvent, make_time\n",
    "import os\n",
    "all = []\n",
    "files=[]\n",
    "for f in os.listdir(\"./data/zimu\"):\n",
    "    remove_fil=\"1850年代出版的一部有历史背景的小说\"\n",
    "    try:\n",
    "        data={}\n",
    "        if f.endswith(\".ass\") or f.endswith(\".srt\") or f.endswith(\".ssa\"):\n",
    "            subs = SSAFile.load('./data/zimu/{}'.format(f))\n",
    "\n",
    "            for line in subs:\n",
    "                time = str(line.start)\n",
    "                text = line.plaintext\n",
    "                tmp = {\"text1\":\"\",\"text2\":\"\"}\n",
    "                \n",
    "                if time not in data:\n",
    "                    tmp[\"text1\"] = text\n",
    "                    data[time] = tmp\n",
    "                else:\n",
    "                    data[time][\"text2\"] = text\n",
    "              #print(line.plaintext)\n",
    "              #print(line.start)\n",
    "            files.append(f)\n",
    "            all.append(data)\n",
    "    \n",
    "    except:\n",
    "        print(\"error\",f,text)\n",
    "        continue\n",
    "\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bc816d76-265f-4a52-a293-58b14884f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_diff_file():\n",
    "    zh=[]\n",
    "    ja=[]\n",
    "    files=[]\n",
    "    for f in os.listdir(\"./data/zimu\"):\n",
    "        try:\n",
    "            data_zh = []\n",
    "            data_ja = []\n",
    "            if f.endswith(\".ass\") or f.endswith(\".srt\") or f.endswith(\".ssa\"):\n",
    "                post_f = f[3:]\n",
    "                \n",
    "                chs = SSAFile.load('./data/zimu/Chs{}'.format(post_f))\n",
    "                jpn = SSAFile.load('./data/zimu/Jpn{}'.format(post_f))\n",
    "              \n",
    "                for ch in chs:\n",
    "                    txt = ch.plaintext\n",
    "                    data_zh.append(txt.replace(\"\\n\",\" \"))\n",
    "                for jn in jpn:\n",
    "                    txt = jn.plaintext\n",
    "                    data_ja.append(txt.replace(\"\\n\",\" \"))\n",
    "                zh.append(data_zh)\n",
    "                ja.append(data_ja)\n",
    "               \n",
    "                files.append(f)\n",
    "                break\n",
    "        except:\n",
    "            print(\"error\",f)\n",
    "            continue\n",
    "    print(len(files))\n",
    "    print(len(zh))\n",
    "    print(len(ja))\n",
    "    return zh,ja\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "db78f194-ed92-402a-a829-fe7a6c9c9094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "['要反抗吗？', '和我一起', '要不要起身反抗？', '（关东地区沉没的可能性）', '什么？']\n",
      "['（天海(あまみ)） 戦ってみないか？', '一緒に', '（天海）戦ってみないか', '（常盤(ときわ)）なんで…', '（東山(ひがしやま)）誰が…']\n"
     ]
    }
   ],
   "source": [
    "chs,jpn = load_diff_file()\n",
    "print(chs[0][:5])\n",
    "print(jpn[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6ca6dfa9-0640-43bb-8b0e-9ddafee44078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ass_zh = []\n",
    "# ass_ja = []\n",
    "# for data in file_data:\n",
    "#     for k,la in data.items():\n",
    "#         if len(la['zh']) > 0 and len(la['ja']):\n",
    "#             ass_zh.append(la['zh'])\n",
    "#             ass_ja.append(la['ja'])\n",
    "# print(len(ass_zh))\n",
    "# print(len(ass_ja))\n",
    "# print(ass_zh[:5])\n",
    "# print(ass_ja[:5])\n",
    "# print(ass_zh[-10:])\n",
    "# print(ass_ja[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f6f136c-974a-485a-9156-81355fd29c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "jap = re.compile(r'[\\u3040-\\u309F\\u30A0-\\u30FF\\uAC00-\\uD7A3]')\n",
    "chinese=re.compile(u'[\\u4e00-\\u9fa5]+')\n",
    "\n",
    "def text_utils(text,lang='ja'):\n",
    "    if lang.lower()=='ja':\n",
    "        jap = re.compile(r'[\\u3040-\\u309F\\u30A0-\\u30FF\\uAC00-\\uD7A3]') #判断韩文跟日文\n",
    "        return bool(jap.search(text))\n",
    "    elif lang.lower() == 'en':\n",
    "        return bool(re.search('[a-z]',text))\n",
    "    elif lang.lower() == 'zh':\n",
    "        chinese=re.compile(u'[\\u4e00-\\u9fa5]+')\n",
    "        return bool(chinese.search(text))\n",
    "        \n",
    "text_utils(\"Now, your meaningless life\",\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5ec9dfd0-1097-45c2-a012-aefa954c1771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "505\n",
      "['在看见什么的一瞬间', '空气流动着', '在杂乱的日常生活中', '呼吸的空气中 飞舞着灰尘', '是因为什么都没有改变的原因吧', '空气都静止了…', '身体疲惫不堪', '对这样的自己 没有任何的感动', '孤独的时候 好想找人说说话', '今天就让我们悠闲的交谈吧', '你展现出这一天', '最平和的表情', '在这疲惫不堪的紧张感中', '展现出最灿烂的时刻', '今天就让我们悠闲的交谈吧', '你展现出这一天', '最平和的表情', '在众多的愿望中', '如果只能实现其中一个的话', '我会选择什么呢…']\n",
      "['そんな何かを見た瞬間に', '空気が動く', '雑然とした日常の中で', '息を吸うたび舞うホコリのように', '何ら 変わっちゃいないせいなのさ', '空気が止まる…', 'くたびれていて', 'そんな自分が無感動で', '人恋しくて 孤独なら', '今日はゆっくり話そう', '君は この日一番の穏やかな', 'その顔を 見せるね', 'すり切れる程の 緊張感の中で', '最も輝くその時を', '今日はゆっくり話そう', '君は この日一番の穏やかな', 'その顔を 見せるね', '数多い願いの中でも', 'たったひとつ叶えられるとしたら', '何を選ぶだろう…']\n",
      "['想在你的梦里和你继续走下去', '夏天悄无声息的来临', '波光粼粼的海浪 浸湿了沙滩', '将周围的一切束缚全都抛开', '现在 就决定是你了', '这样自己没有合适的另一半', '心曾死了一半', '摇曳的思绪      荡漾在心头', '想要像现在这样一直陪在你身边', '就像那湛蓝而又清澈的天空', '想在你的梦里和你继续走下去', '你的眼眸深处传递出了喜欢的讯息', '在假装看你双眸的时候 偷吻一下', '因为害怕将自己完全暴露在你的面前', '所以有时也会逃避你的温柔', '这是命运的相逢 毋庸置疑', '自己竟也开始改变', '摇曳的思绪      荡漾在心头']\n",
      "['君と歩き続けたい in your dream', '夏が忍び足で 近づくよ', 'きらめく波が 砂浜 潤して', 'こだわってた周囲を すべて捨てて', '今 あなたに決めたの', 'こんな自分に合う人はもう', 'いないと半分あきらめてた', '揺れる想い体じゅう感じて', 'このままずっとそばにいたい', '青く澄んだあの空のような', '君と歩き続けたい in your dream', '好きと合図送る 瞳の奥', '覗いてみる振りして キスをした', 'すべてを見せるのが 怖いから', 'やさしさから逃げてたの', '運命の出逢い 確かね こんなに', '自分が 変わってくなんて', '揺れる想い体じゅう感じて']\n"
     ]
    }
   ],
   "source": [
    "# print(all[0])\n",
    "# print(files[0])\n",
    "\n",
    "zh=[]\n",
    "ja=[]\n",
    "import re\n",
    "jap = re.compile(r'[\\u3040-\\u309F\\u30A0-\\u30FF\\uAC00-\\uD7A3]')\n",
    "chinese=re.compile(u'[\\u4e00-\\u9fa5]+')\n",
    "src_lang='ja'\n",
    "  \n",
    "\n",
    "for it in all:\n",
    "    for k,v in it.items():\n",
    "        if text_utils(v['text1'],src_lang) and len(v[\"text2\"]) > 0:\n",
    "            txt = v['text1']\n",
    "            wen=txt.replace(\"\\u3000\",\" \")\n",
    "            \n",
    "            items = wen.split(\"\\n\")\n",
    "            \n",
    "            if len(items) == 2:\n",
    "                if text_utils(items[0],src_lang) and not text_utils(items[0],'zh'):\n",
    "                 \n",
    "                    ja.append(items[0])\n",
    "                    zh.append(items[1])\n",
    "                else:\n",
    "                    \n",
    "                    ja.append(items[1])\n",
    "                    zh.append(items[0])\n",
    "        \n",
    "            if len(items) ==1 and chinese.search(v[\"text2\"]):\n",
    "                ja.append(wen)\n",
    "                zh.append(v['text2'])\n",
    "                # print(v['text2'])\n",
    "            \n",
    "    \n",
    "                \n",
    "        elif text_utils(v['text1'],src_lang) and len(v[\"text2\"]) == 0:\n",
    "            txt = v['text1']\n",
    "            wen=txt.replace(\"\\u3000\",\" \")\n",
    "            items = wen.split(\"\\n\")\n",
    "            \n",
    "            if len(items) == 2:\n",
    "                if text_utils(items[0],src_lang): #and not text_utils(items[0],'zh')\n",
    "                 \n",
    "                    ja.append(items[0])\n",
    "                    zh.append(items[1])\n",
    "                else:\n",
    "                    print(items[0])\n",
    "                    ja.append(items[1])\n",
    "                    zh.append(items[0])\n",
    "                    \n",
    "            if len(items) > 2:\n",
    "                if text_utils(items[0],'zh') and not text_utils(items[0],src_lang):\n",
    "                    print(items)\n",
    "                    zh.append(items[0])\n",
    "                    ja.append(\"\".join(items[1:]))\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            wen=v['text2'].replace(\"\\u3000\",\" \")\n",
    "            if text_utils(wen,src_lang):\n",
    "                # print(\"hello\")\n",
    "                if len(wen) > 0:\n",
    "                    ja.append(wen)\n",
    "                    zh.append(v['text1'])\n",
    "\n",
    "\n",
    "\n",
    "print(len(zh) == len(ja))\n",
    "print(len(zh))\n",
    "print(zh[-20:])\n",
    "print(ja[-20:])\n",
    "print(zh[2:20])\n",
    "print(ja[2:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7888d159-42c1-488a-8fa7-f2513be75864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "aa3f74da-4436-4e42-868c-9c0da929b02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--==本影片由 CMCT 團隊 榮譽出品==--', '--==本影片由 CMCT 团队 荣誉出品==--']\n",
      "['更多精彩影視 請訪問 http://cmct.cc', '更多精彩影视 请访问 http://cmct.cc']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "zh=[]\n",
    "ja=[]\n",
    "import re\n",
    "jap = re.compile(r'[\\u3040-\\u309F\\u30A0-\\u30FF\\uAC00-\\uD7A3]')\n",
    "\n",
    "for it in all[:2]:\n",
    "    for k,v in it.items():\n",
    "        \n",
    "        if text_utils(v['text1'],'en'):\n",
    "            txt = v['text1'].split(\"\\n\")\n",
    "        \n",
    "        if len(txt) == 2:\n",
    "            if text_utils(txt[0],'en'):\n",
    "                wen=txt[0].replace(\"\\u3000\",\" \")\n",
    "                ja.append(wen)\n",
    "                zh.append(txt[1])\n",
    "            else:\n",
    "                wen=txt[1].replace(\"\\u3000\",\" \")\n",
    "                ja.append(wen)\n",
    "                zh.append(txt[0])\n",
    "print(zh[:10])\n",
    "print(ja[:10])\n",
    "print(len(ja)==len(zh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be06cd96-ecaf-48c4-9d93-556dd3350306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['碞稰谋硂赣']\n"
     ]
    }
   ],
   "source": [
    "from opencc import OpenCC\n",
    "zh = ['碞稰谋硂赣']\n",
    "jianti = [OpenCC(\"t2s\").convert(s) for s in zh]\n",
    "print(jianti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "eb66a67f-ab7b-4f60-93de-9da5c2f2d057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['摇曳的思绪', '摇曳的思绪      荡漾在心头', '想在你的梦里和你继续走下去', '夏天悄无声息的来临', '波光粼粼的海浪 浸湿了沙滩', '将周围的一切束缚全都抛开', '现在 就决定是你了', '这样自己没有合适的另一半', '心曾死了一半', '摇曳的思绪      荡漾在心头', '想要像现在这样一直陪在你身边', '就像那湛蓝而又清澈的天空', '想在你的梦里和你继续走下去', '你的眼眸深处传递出了喜欢的讯息', '在假装看你双眸的时候 偷吻一下', '因为害怕将自己完全暴露在你的面前', '所以有时也会逃避你的温柔']\n",
      "['揺れる想い', '揺れる想い体じゅう感じて', '君と歩き続けたい in your dream', '夏が忍び足で 近づくよ', 'きらめく波が 砂浜 潤して', 'こだわってた周囲を すべて捨てて', '今 あなたに決めたの', 'こんな自分に合う人はもう', 'いないと半分あきらめてた', '揺れる想い体じゅう感じて', 'このままずっとそばにいたい', '青く澄んだあの空のような', '君と歩き続けたい in your dream', '好きと合図送る 瞳の奥', '覗いてみる振りして キスをした', 'すべてを見せるのが 怖いから', 'やさしさから逃げてたの']\n",
      "['你展现出这一天', '最平和的表情', '在这疲惫不堪的紧张感中', '展现出最灿烂的时刻', '今天就让我们悠闲的交谈吧', '你展现出这一天', '最平和的表情', '在众多的愿望中', '如果只能实现其中一个的话', '我会选择什么呢…']\n",
      "['君は この日一番の穏やかな', 'その顔を 見せるね', 'すり切れる程の 緊張感の中で', '最も輝くその時を', '今日はゆっくり話そう', '君は この日一番の穏やかな', 'その顔を 見せるね', '数多い願いの中でも', 'たったひとつ叶えられるとしたら', '何を選ぶだろう…']\n",
      "505\n"
     ]
    }
   ],
   "source": [
    "tgt_a = []\n",
    "src_a = []\n",
    "jianti=zh\n",
    "for i in range(len(jianti)):\n",
    "    if \"翻译\" not in ja[i]:\n",
    "        if \"第5集中也曾\" not in ja[i]:\n",
    "            tgt_a.append(jianti[i].replace(\"\\ufeff\",\"\").replace(\"♪\",\"\").replace(\"\\t\",\"\"))\n",
    "            src_a.append(ja[i].replace(\"\\ufeff\",\"\").replace(\"♪\",\"\").replace(\"\\t\",\"\"))\n",
    "print(len(tgt_a)==len(src_a))\n",
    "print(tgt_a[:17])\n",
    "print(src_a[:17])\n",
    "print(tgt_a[-10:])\n",
    "print(src_a[-10:])\n",
    "print(len(src_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6951b50-be11-4a37-b7ec-cea14613e1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548\n",
      "['Ryokee\\nりょうっき', 'マジでやるんだな', 'どこからがいい メガネ', 'つまらんヤツだ', 'それが IBM', 'どういうことだ', 'そうだったのか', 'ちくしょう', 'テレビもくれ', 'おい こらっ', 'どうすんだよ', 'もういい ずっとそうやってろ', 'よし じゃあついてこい', 'ひょっとしてこいつ…', 'ありがとう', 'よかった…かね', 'そうなんだ', 'うん', 'におうんだよ', 'バカにしてるのか', 'ダメです', 'そ…それは…', 'どうだった', 'となると やはり…', 'はい', 'コメント それだけですか', 'ここは…', 'こんにちはー', 'マズいな', 'それは…']\n",
      "['\\n时间轴/特效', '真要干啊', '眼镜兄 你想先听什么', '无趣的家伙', '那就是IBM', '这是什么意思', '原来是这样', '可恶', '再给我弄台电视来', '喂 别走', '你到底想怎样', '好好好 你就一直这样吧', '好 那跟我来吧', '难道说这家伙…', '谢谢您', '以前…挺好的', '原来是这样', '嗯', '我能闻到', '你把我们当傻子吗', '这可不行', '那…那是…', '怎么样', '这么说 果然…', '是的', '您要说的只有这些吗', '这里是…', '您好', '糟了', '那是因为…']\n",
      "['ケイ', 'なに', 'コレってやっぱりこういうことなんだ', 'しくったぜ', 'うん では', 'はい？', 'はッ', 'はい', 'どうした', 'あ…いや', 'いきなりこんなことになるなんて', 'そんなこと…それよりケガ…', 'クソッ', 'ついてる', 'だよな', 'おい ふざけんな', 'エンストか', 'なんだろう', 'さっきのでハッキリした', 'すごく…おしっこしたい']\n",
      "['圭', '怎么回事', '果然是这么一回事', '搞砸了', '好 失陪了', '什么？', '是', '是', '怎么了', '啊…没事', '没想到会发生这种事', '没事…比起这个 你的伤…', '可恶', '跟过来了', '当然了', '喂 别开玩笑了', '熄火了吗', '那边怎么了', '刚才的事让我明白了', '好想…小便']\n"
     ]
    }
   ],
   "source": [
    "src_b = []\n",
    "tgt_b = []\n",
    "for id,en in enumerate(src_a):\n",
    "    if text_utils(en,\"zh\"):\n",
    "        its = en.split(\"\\n\")\n",
    "        if len(its) == 2:\n",
    "            if text_utils(its[0],src_lang):\n",
    "                src_b.append(its[0])\n",
    "                tgt_b.append(its[1])\n",
    "            else:\n",
    "                src_b.append(its[1])\n",
    "                tgt_b.append(its[0])\n",
    "     \n",
    "    else:\n",
    "        src_b.append(src_a[id])\n",
    "        tgt_b.append(tgt_a[id])\n",
    "\n",
    "print(len(src_b))\n",
    "print(src_b[:30])\n",
    "print(tgt_b[:30])\n",
    "print(src_b[-20:])\n",
    "print(tgt_b[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1deec-4301-48ec-a7aa-867775d83996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "07684a3c-24a8-4b77-b77b-bcde50582a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b4ac440e-e5f8-4c91-bdf5-e3d6a0f26023",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/videozimu/xunchang.ja\",\"a+\") as f:\n",
    "    for i in src_a:\n",
    "        f.write(i.replace(\"\\n\",\"\")+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b76f30e2-eeba-4b11-bfff-6acf3d7e476d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "40000\n",
      "[\"There's hundreds of them. Shall we go get them?\", 'Yeah. This will be fun.', 'Yeah, absolutely.', 'He makes it look so easy on those ski-doos.', \"But in virgin snow, you know, it's incredibly difficult,\", \"because the slower you go, the more you're gonna fall off.\", \"Oh, that's hard!\", \"You've gotta get speed\", 'to get momentum to sit on top of that snow.', \"It looks easy, but it's bloody tough.\"]\n",
      "['得有几百只吧  我们去抓吧', '是的  会很有趣的', '肯定的', '看他骑雪地摩托感觉很轻松', '但在没人走过的雪地里  行动非常困难', '因为你开得越慢  就越容易摔下来', '这太难了', '你必须得有速度', '才能获得在雪上骑行的足够动力', '看起来简单  但十分困难']\n",
      "58145\n",
      "58145\n"
     ]
    }
   ],
   "source": [
    "pr_en_file=\"./data/en-zh/{}\".format(\"en-zh.en\")\n",
    "pr_zh_file=\"./data/en-zh/{}\".format(\"en-zh.zh\")\n",
    "with open(pr_en_file) as f:\n",
    "    src_b = f.read().splitlines()\n",
    "with open(pr_zh_file) as f:\n",
    "    tgt_b = f.read().splitlines()\n",
    "src_all_b=[i.strip().replace('\"',\"\") for i in src_b]\n",
    "tgt_all_b=[i.strip() for i in tgt_b]\n",
    "src_b = src_all_b[:40000]\n",
    "tgt_b = tgt_all_b[:40000]\n",
    "print(len(src_b))\n",
    "print(len(tgt_b))\n",
    "print(src_b[:10])\n",
    "print(tgt_b[:10])\n",
    "ori_en = src_all_b[40000:]\n",
    "ori_zh = tgt_all_b[40000:]\n",
    "print(len(ori_en))\n",
    "print(len(ori_zh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0925770-2a12-4006-b645-8775dd948eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32495\n",
      "True\n",
      "['得有几百只吧  我们去抓吧', '是的  会很有趣的', '肯定的', '看他骑雪地摩托感觉很轻松', '但在没人走过的雪地里  行动非常困难 因为你开得越慢  就越容易摔下来', '这太难了', '你必须得有速度 才能获得在雪上骑行的足够动力', '看起来简单  但十分困难', '上到山顶上之后 风突然变大了', '我在挪威美丽的西海岸上', '这里峡湾遍布  寒冷刺骨 还有身材健硕的北欧海盗', '我一直很喜欢这个 伟大国家生产的海鲜', '所以我在十二月份来到了这里', '大厨告诉我这个时候是挪威海鲜 和其他美味佳肴的巅峰时期', '所以我要唤起我内心的维京人 在这寒冷的温度里生存下去', '我该开始了  因为现在这里的平均日照 只有大约五个小时', '该出发了', '我的挪威冒险 始于卑尔根外的小岛上 就在挪威的西南海岸 我和克里斯托弗·哈图夫主厨 在他最爱的觅食点碰面', '这里太漂亮了', '欢迎']\n",
      "[\"There's hundreds of them. Shall we go get them?\", 'Yeah. This will be fun.', 'Yeah, absolutely.', 'He makes it look so easy on those ski-doos.', \"But in virgin snow, you know, it's incredibly difficult, because the slower you go, the more you're gonna fall off.\", \"Oh, that's hard!\", \"You've gotta get speed to get momentum to sit on top of that snow.\", \"It looks easy, but it's bloody tough.\", 'You get to the top of the ridge on that mountain, and the wind just kicks in.', \"I'm in norway on the beautiful west coast.\", 'Home to fjords, freezing temperatures, and bloody big Vikings.', \"Now, I've always been so fascinated by the seafood that this amazing country produces.\", \"Which is why I'm here in December.\", \"Chefs tell me that this is when Norway's seafood and other delicacies are at their peak.\", \"So I'm gonna have to channel my inner Viking to survive these freezing temperatures.\", \"I'd better get started, because on an average day they only have around five hours of daylight.\", 'Time to move my butt.', \"To kick off my adventure, I'm traveling to a small island just outside Bergen, on Norway's southwestern coast to meet Chef Christopher Haatuft at his favorite foraging spot.\", 'This is beautiful.', 'Welcome.']\n",
      "['我想塞伯特校长的意思是 你们有机会赢得诺贝尔奖 现在要被你们搞黄了', '这还是他的原话啊', '是啊  但是用我平静的人事部嗓音说的', '我想很明显 我突然暴怒真的很对不起', '我叫你们骗子的事情真的很不对', '我心里也觉得很抱歉', '这事情真的让她很煎熬', '她压力过大  磨牙都咬穿保持器了 想咬牛肉干似的', '你说的话真的很伤人 尤其我一直觉得我们是朋友', '你怎么会这么想', '伤上加伤了', '别理他们  他们就是很恶毒', '不恶毒  只是很过意不去罢了', '看似恶毒  实则抱歉  这事常有', '没事啦  我跟我的心理治疗师聊过了 她开导我  让我理解你的话 其实真正反映的是你内心的不安全感', '不好意思  你说什么来着', '听到啦  她很抱歉  在座都听到了  向前看吧', '是啊  很明显 你生气是因为我们的发现令大家把目光 都聚集在我们身上  所以你们发飙了', '正确来说  是她发飙了', '我控制好了自己  所以我觉得你们']\n",
      "[\"I think what President Siebert is trying to say is that you have a shot to win a Nobel Prize, and you're blowing it.\", \"Uh, that's exactly what he said.\", 'Yes, but I said it in my calming HR voice.', \"So, obviously, I'm really sorry about my outburst.\", 'Calling you frauds was unacceptable.', 'I feel terrible about it.', 'It has caused her a lot of stress.', 'She chewed through her night guard like it was a piece of jerky.', 'What you said was really hurtful, especially because I thought we were friends.', 'Why would you think that?', 'Ouch.', \"Ignore them; they're just mean people.\", 'Uh, not mean, just sorry.', 'Seem mean, are sorry. Happens all the time.', \"It's okay. I talked to my therapist, and she made me realize that what you said was really more about your own insecurities.\", \"I'm sorry, what?\", \"There you go, she's sorry. We all heard it. Moving on.\", \"Yes, obviously, you're angry at all the attention we're getting for our discovery and you're lashing out.\", 'Well, uh, technically, she lashed out.', \"I contained myself, which I don't think\"]\n"
     ]
    }
   ],
   "source": [
    "tgt_c=[]\n",
    "src_c=[]\n",
    "src_tmp = \"\"\n",
    "tgt_tmp = \"\"\n",
    "for i,en in enumerate(src_b):\n",
    "    \n",
    "    en = en.replace(\"...\",\"\")\n",
    "    if en.endswith(\".\") or en.endswith(\"?\") or en.endswith(\"!\"):\n",
    "        \n",
    "        tgt_tmp = tgt_tmp + \" \" + tgt_b[i]\n",
    "        src_tmp = src_tmp + \" \" + en\n",
    "        tgt_c.append(tgt_tmp)\n",
    "        src_c.append(src_tmp)\n",
    "        src_tmp = \"\"\n",
    "        tgt_tmp = \"\"\n",
    "    else:\n",
    "        tgt_tmp = tgt_tmp + \" \" + tgt_b[i]\n",
    "        src_tmp = src_tmp + \" \" + en\n",
    "        \n",
    "if len(tgt_tmp) > 0:\n",
    "    tgt_c.append(tgt_tmp)\n",
    "    src_c.append(src_tmp)\n",
    "        \n",
    "#     if en.startswith(\"of\") or en.startswith(\"and\") or en.startswith(\"or\"):\n",
    "#         tgt_tmp = tgt_tmp + \" \" + tgt_b[i]\n",
    "#         src_tmp = src_tmp + \" \" + en\n",
    "    \n",
    "src_c=[i.strip().replace('\"',\"\") for i in src_c]\n",
    "tgt_c=[i.strip() for i in tgt_c]\n",
    "print(len(tgt_c))\n",
    "print(len(tgt_c)==len(src_c))\n",
    "print(tgt_c[:20])\n",
    "print(src_c[:20])\n",
    "print(tgt_c[-20:])\n",
    "print(src_c[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "160697d7-6a13-4813-9ad0-d0c2105d0588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "[5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4,5,6,7]\n",
    "step=4\n",
    "for i in range(0,len(a),step):\n",
    "    x = a[i:i+step]\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py3",
   "language": "python",
   "name": "env_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
