{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f2b40-c10f-4d51-a674-9e549d338492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import jieba\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "Past = Tuple[Tensor, Tensor]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, middle_dim = 2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.PE = pe\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x += Variable(self.pe[:, :x.size(1)],requires_grad=False)        \n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class BaseAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Tensor          Type            Shape\n",
    "    ===========================================================================\n",
    "    q               float           (..., query_len, dims)\n",
    "    k               float           (..., kv_len, dims)\n",
    "    v               float           (..., kv_len, dims)\n",
    "    mask            bool            (..., query_len, kv_len)\n",
    "    ---------------------------------------------------------------------------\n",
    "    output          float           (..., query_len, dims)\n",
    "    ===========================================================================\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dropout: float = 0.1, scale=True):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                q: Tensor,\n",
    "                k: Tensor,\n",
    "                v: Tensor,\n",
    "                mask: Optional[Tensor] = None) -> Tensor:\n",
    "\n",
    "        x = torch.matmul(q, k.transpose(-2, -1))\n",
    "        if self.scale: x = x / math.sqrt(k.size(-1))\n",
    "        if mask is not None:\n",
    "            x += mask.type_as(x) * x.new_tensor(-1e6)\n",
    "        x = self.dropout(x.softmax(-1))\n",
    "\n",
    "        return torch.matmul(x, v)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(BaseAttention):\n",
    "    \"\"\"\n",
    "    Tensor          Type            Shape\n",
    "    ===========================================================================\n",
    "    q               float           (..., query_len, dims)\n",
    "    k               float           (..., kv_len, dims)\n",
    "    v               float           (..., kv_len, dims)\n",
    "    mask            bool            (..., query_len, kv_len)\n",
    "    ---------------------------------------------------------------------------\n",
    "    output          float           (bs, query_len, dims)\n",
    "    ===========================================================================\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, heads: int, dropout: float = 0.1):\n",
    "        super().__init__(dropout)\n",
    "        self.heads = heads\n",
    "\n",
    "    def forward(self,\n",
    "                q: torch.Tensor,\n",
    "                k: torch.Tensor,\n",
    "                v: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # Split the tensors to multi-heads.\n",
    "        q = q.view(q.size()[:-1] + (self.heads, q.size(-1) // self.heads))  # [batch_size, query_len, heads, dim]\n",
    "        k = k.view(k.size()[:-1] + (self.heads, k.size(-1) // self.heads))  # [batch_size, key_len, heads, dim]\n",
    "        v = v.view(v.size()[:-1] + (self.heads, v.size(-1) // self.heads))  # [batch_size, key_len, heads, dim]\n",
    "\n",
    "        q = q.transpose(-3, -2)  # [batch_size, heads, query_len, dim]\n",
    "        k = k.transpose(-3, -2)\n",
    "        v = v.transpose(-3, -2)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(-3)  # [batch_size,1, query_len, key_len]\n",
    "\n",
    "        # Calculate multi-headed attentions and merge them into one.\n",
    "        return (super().forward(q, k, v, mask)\n",
    "                .transpose(-3, -2)\n",
    "                .contiguous()\n",
    "                .view(q.size()[:-3] + (q.size(-2), v.size(-1) * self.heads)))\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Tensor          Type            Shape\n",
    "    ===========================================================================\n",
    "    q               float           (..., query_len, dims)\n",
    "    k               float           (..., kv_len, dims)\n",
    "    v               float           (..., kv_len, dims)\n",
    "    past (*)        float           (..., past_len, dims)\n",
    "    mask            bool            (..., query_len, past_len + kv_len)\n",
    "    ---------------------------------------------------------------------------\n",
    "    output 1        float           (..., query_len, dims)\n",
    "    output 2 (*)    float           (..., past_len + kv_len, dims)\n",
    "    ===========================================================================\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, heads: int, dims: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        d_head, remainder = divmod(dims, heads)\n",
    "        if remainder:\n",
    "            raise ValueError(\" incompatible `dims` and `heads` \")\n",
    "        self.attn = MultiHeadAttention(heads, dropout)\n",
    "        self.proj_q = nn.Linear(dims, dims)\n",
    "        self.proj_k = nn.Linear(dims, dims)\n",
    "        self.proj_v = nn.Linear(dims, dims)\n",
    "        self.linear = nn.Linear(dims, dims)\n",
    "\n",
    "    def forward(self,\n",
    "                q: torch.Tensor,\n",
    "                k: torch.Tensor,\n",
    "                v: torch.Tensor,\n",
    "                past: Optional[Past] = None,\n",
    "                mask: Optional[Tensor] = None\n",
    "                ) -> Tuple[torch.Tensor, Past]:\n",
    "        q, k, v = self.proj_q(q), self.proj_k(k), self.proj_v(v)\n",
    "\n",
    "        # Reuse attention keys and values by concatenating to the current ones.\n",
    "        if past is not None:\n",
    "            k = torch.cat((past[0], k), dim=-2)\n",
    "            v = torch.cat((past[1], v), dim=-2)\n",
    "\n",
    "        x = self.linear(self.attn(q, k, v, mask))\n",
    "        return x, (k, v)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.self_multihead = AttentionLayer(heads,d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        att_out, _ = self.self_multihead(embeddings, embeddings, embeddings, mask=mask)\n",
    "        interacted = self.dropout(att_out)\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.dec_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = AttentionLayer(heads, d_model)\n",
    "        self.src_multihead = AttentionLayer(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
    "        att_target_out, _ = self.self_multihead(embeddings, embeddings, embeddings, mask=target_mask)\n",
    "        query = self.dropout(att_target_out)\n",
    "        query = self.layernorm(query + embeddings)\n",
    "        en_den_att, _ = self.src_multihead(query, encoded, encoded, mask=src_mask)\n",
    "        interacted = self.dropout(en_den_att)\n",
    "        interacted = self.layernorm(interacted + query)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        decoded = self.dec_layer_norm(feed_forward_out + interacted)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads, num_layers, src_vocab_size,target_vocab_size):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = target_vocab_size\n",
    "        # self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.src_embed = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embed = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.srcpe = PositionalEncoding(d_model,0, src_vocab_size)\n",
    "        self.tgtpe = PositionalEncoding(d_model,0, target_vocab_size)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
    "        self.src_masking = PadMasking(pad_or_ahead=\"pad\",idx=0)\n",
    "        self.tgt_masking = PadMasking(pad_or_ahead=\"ahead\",idx=0)\n",
    "        # self.cross_masking = PadMasking(pad_or_ahead=\"ahead\",idx=0)\n",
    "        \n",
    "    def encode(self, src_words, src_mask):\n",
    "        src_embeddings = self.src_embed(src_words) * math.sqrt(self.d_model)\n",
    "        src_embeddings = self.srcpe(src_embeddings)\n",
    "        for layer in self.encoder:\n",
    "            src_embeddings = layer(src_embeddings, src_mask)\n",
    "        return src_embeddings\n",
    "    \n",
    "    def decode(self, target_words, target_mask, src_embeddings, src_mask):\n",
    "        tgt_embeddings = self.tgt_embed(target_words) * math.sqrt(self.d_model)\n",
    "        tgt_embeddings = self.tgtpe(tgt_embeddings)\n",
    "        for layer in self.decoder:\n",
    "            tgt_embeddings = layer(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
    "        return tgt_embeddings\n",
    "        \n",
    "    def forward(self, src_words, target_words,training=True):\n",
    "        src_mask = self.src_masking(src_words) if training else None\n",
    "        target_mask = self.tgt_masking(target_words) if training else None\n",
    "        \n",
    "        encoded = self.encode(src_words, src_mask)\n",
    "        # print(encoded.shape)\n",
    "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
    "        out = F.log_softmax(self.logit(decoded), dim = -1)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class PadMasking(nn.Module):\n",
    "    def __init__(self, pad_or_ahead=\"ahead\", idx=0):\n",
    "        \"\"\"\n",
    "        :param pad_or_ahead: 选择哪种masking的方式 只有以下两种方式\n",
    "        \"\"\"\n",
    "        super(PadMasking, self).__init__()\n",
    "        self.pad_or_ahead = pad_or_ahead\n",
    "        self.idx = idx\n",
    "\n",
    "    def create_padding_mask(self, x: torch.Tensor, idx=0):\n",
    "        \"\"\"\n",
    "        input shape: [batch_size, seq_len]\n",
    "        return [batch_size,1,1,seq_len]\n",
    "        :param idx token 为PAD的id值\n",
    "        \"\"\"\n",
    "        # zeros = torch.zeros_like(x)\n",
    "        mask = torch.eq(x, idx).type(torch.float32).to(x.device)\n",
    "        return mask[:, None, :]\n",
    "\n",
    "    def create_look_ahead_mask(self, x, idx=0):\n",
    "        \"\"\"\n",
    "        input_shape:[batch_size, seq_len]\n",
    "        return : [batch_size, 1, seq_len, seq_len]\n",
    "        掩盖后面的token\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        look_ahead_mask = 1 - torch.tril(torch.ones(seq_len, seq_len), diagonal=0)\n",
    "        look_ahead_mask = look_ahead_mask.to(x.device)\n",
    "        padding_mask = self.create_padding_mask(x, idx)\n",
    "        return torch.maximum(look_ahead_mask, padding_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.pad_or_ahead == \"pad\":\n",
    "            return self.create_padding_mask(x, self.idx)\n",
    "        elif self.pad_or_ahead == \"ahead\":\n",
    "            return self.create_look_ahead_mask(x, self.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fb4af6f-f7da-41df-a690-6aca387d13fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWarmup:\n",
    "    \n",
    "    def __init__(self, model_size, warmup_steps, optimizer):\n",
    "        \n",
    "        self.model_size = model_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.optimizer = optimizer\n",
    "        self.current_step = 0\n",
    "        self.lr = 0\n",
    "        \n",
    "    def get_lr(self):\n",
    "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
    "        \n",
    "    def step(self):\n",
    "        # Increment the number of steps each time we call the step function\n",
    "        self.current_step += 1\n",
    "        lr = self.get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        # update the learning rate\n",
    "        self.lr = lr\n",
    "        self.optimizer.step()   \n",
    "\n",
    "\n",
    "class LossWithLS(nn.Module):\n",
    "\n",
    "    def __init__(self, size, smooth):\n",
    "        super(LossWithLS, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"none\")\n",
    "        self.confidence = 1.0 - smooth\n",
    "        self.smooth = smooth\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, prediction, target):\n",
    "        \"\"\"\n",
    "        prediction of shape: (batch_size, max_words, vocab_size)\n",
    "        target and mask of shape: (batch_size, max_words)\n",
    "        \"\"\"\n",
    "        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n",
    "        target = target.contiguous().view(-1)   # (batch_size * max_words)\n",
    "        \n",
    "        mask = (target > 0).float()       # (batch_size * max_words)\n",
    "        labels = prediction.data.clone()\n",
    "        labels.fill_(self.smooth / (self.size - 1))\n",
    "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n",
    "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22fb4c07-acf1-4508-84ce-8b579e1c8aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import jieba\n",
    "import json\n",
    "import os\n",
    "import yaml\n",
    "def load_yaml_text(file_path):\n",
    "   \n",
    "    with open(file_path) as f:\n",
    "        lines = f.read()\n",
    "    lines = yaml.safe_load(lines)\n",
    "    return lines\n",
    "\n",
    "\n",
    "def load_all_data(directory):\n",
    "    \n",
    "    file_names = os.listdir(directory)\n",
    "    questions, answers = [], []\n",
    "    for f_name in file_names:\n",
    "        if f_name.endswith(\".yml\"):\n",
    "            file_path = os.path.join(directory, f_name)\n",
    "            conversions = load_yaml_text(file_path)['conversations']\n",
    "            ques = [i[0] for i in conversions]\n",
    "            ans = [i[1] for i in conversions]\n",
    "            questions.extend(ques)\n",
    "            answers.extend(ans)\n",
    "    print(\"load data finished,the size of data is \", len(questions))\n",
    "    questions = [\" \".join(jieba.cut(str(s), cut_all=False, HMM=True)) for s in questions]\n",
    "    answers = [\" \".join(jieba.cut(str(s), cut_all=False, HMM=True)) for s in answers]\n",
    "    return questions, answers\n",
    "\n",
    "#questions, answers = load_all_data(\"../chatterbot-corpus/chatterbot_corpus/data/chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "788b065b-203f-41c3-af9c-ac76126e625c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train===\n",
      "['\\ufeff す', '興奮', '大佐']\n",
      "['\\ufeff 的', '兴奋', '上校']\n",
      "test====\n",
      "['天下 静謐 の ため \\u3000 一層 励む よう に と 。 ', '勅命 を 頂い た の じゃ 。 \\u3000 戦 の 勅命 を ！ ', '永禄 １３ 年 ４ 月 \\u3000 織田 信長 は 諸国 の 兵 を 従え ']\n",
      "['为了 天下 静谧 要 多加 努力 ', '我 得到 了 敕命 开战 的 敕命 ', '1570 年 4 月 织田信长 统领 诸国 军队 ']\n",
      "src_len is equal target_len: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import nltk\n",
    "\n",
    "def load_trans_data(data_dir):\n",
    "    with open(data_dir) as f:\n",
    "        lines_train = f.read().splitlines()\n",
    "    source = [l.split(\"\\t\")[0] for l in lines_train]\n",
    "    target = [l.split(\"\\t\")[1] for l in lines_train]\n",
    "    return source,target\n",
    "\n",
    "def ja_zh_data(data_file):\n",
    "    with open(data_file) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    return lines\n",
    "    \n",
    "# train_en,train_zh = load_trans_data(\"./data/train.txt\")\n",
    "# test_en,test_zh = load_trans_data(\"./data/test.txt\")\n",
    "# dev_en,dev_zh = load_trans_data(\"./data/dev.txt\")\n",
    "\n",
    "\n",
    "# train_en = [\" \".join(nltk.word_tokenize(l)).lower() for l in train_en]\n",
    "# test_en = [\" \".join(nltk.word_tokenize(l)).lower() for l in test_en]\n",
    "# dev_en = [\" \".join(nltk.word_tokenize(l)).lower() for l in dev_en]\n",
    "\n",
    "# train_zh = [\" \".join(jieba.cut(str(s), cut_all=False, HMM=True)) for s in train_zh]\n",
    "# test_zh = [\" \".join(jieba.cut(str(s), cut_all=False, HMM=True)) for s in test_zh]\n",
    "# dev_zh = [\" \".join(jieba.cut(str(s), cut_all=False, HMM=True)) for s in dev_zh]\n",
    "\n",
    "train_ja = ja_zh_data(\"../translation/data/finall/train.ja\")\n",
    "train_zh = ja_zh_data(\"../translation/data/finall/train.zh\")\n",
    "\n",
    "test_ja = ja_zh_data(\"../translation/data/finall/test.ja\")\n",
    "test_zh = ja_zh_data(\"../translation/data/finall/test.zh\")\n",
    "\n",
    "print(\"train===\")\n",
    "print(train_ja[:3])\n",
    "print(train_zh[:3])\n",
    "print(\"test====\")\n",
    "print(test_ja[:3])\n",
    "print(test_zh[:3])\n",
    "\n",
    "src_en = train_ja + test_ja \n",
    "target_zh = train_zh + test_zh \n",
    "print(\"src_len is equal target_len:\",len(src_en) == len(target_zh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a101ceae-7dc7-4f63-b838-18afd95cf0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"there 's hundreds of them . shall we go get them ?\", 'yeah . this will be fun .', 'yeah , absolutely .']\n",
      "['得 有 几百只 吧 我们 去 抓 吧', '是 的 会 很 有趣 的', '肯定 的']\n",
      "load data finished..\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer,tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "import json\n",
    "import torch\n",
    "import jieba\n",
    "with open(\"../translation/data/en-zh/en.txt\") as f:\n",
    "    zimu_src_en = f.read().splitlines()\n",
    "with open(\"../translation/data/en-zh/zh.txt\") as f:\n",
    "    zimu_target_zh = f.read().splitlines()\n",
    "print(zimu_src_en[:3])\n",
    "print(zimu_target_zh[:3])\n",
    "zimu_src_en = [\" \".join(nltk.word_tokenize(i.lower())) for i in zimu_src_en]\n",
    "zimu_target_zh = [\" \".join(jieba.cut(str(s.replace(\" \",\"\")), cut_all=False, HMM=True)) for s in zimu_target_zh]\n",
    "# print(target_zh[:3])\n",
    "\n",
    "# train_en = src_en[:4000]\n",
    "# train_zh = target_zh[:4000]\n",
    "# test_en = src_en[-1000:]\n",
    "# test_zh = target_zh[-1000:]\n",
    "# with open(\"../tmp/token/words_zh.json\") as f:\n",
    "#     data = json.load(f)\n",
    "#     tokenizer_zh = tokenizer_from_json(data)\n",
    "\n",
    "# with open(\"../tmp/token/words_en.json\") as f:\n",
    "#     data1 = json.load(f)\n",
    "#     tokenizer_en = tokenizer_from_json(data1)\n",
    "\n",
    "# id2word =tokenizer_zh.index_word\n",
    "print(\"load data finished..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adabbbf4-21d8-4dec-a09e-57561e7e73ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['得 有 几百只 吧 我们 去 抓 吧', '是 的 会 很 有趣 的', '肯定 的']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zimu_target_zh[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46136980-cafa-4862-bc62-7135535b5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer_en.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45b3abc1-d97c-4ce1-9bc7-7e9d89220e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1929 or 1989 ?', 'paris – as the economic crisis deepens and widens , the world has been searching for historical analogies to help us understand what has been happening .', 'at the start of the crisis , many people likened it to 1982 or 1973 , which was reassuring , because both dates refer to classical cyclical downturns .']\n",
      "['1929 年 还 是 1989 年 ?', '巴 黎 - 随 着 经 济 危 机 不 断 加 深 和 蔓 延 ， 整 个 世 界 一 直 在 寻 找 历 史 上 的 类 似 事 件 希 望 有 助 于 我 们 了 解 目 前 正 在 发 生 的 情 况 。', '一 开 始 ， 很 多 人 把 这 次 危 机 比 作 1982 年 或 1973 年 所 发 生 的 情 况 ， 这 样 得 类 比 是 令 人 宽 心 的 ， 因 为 这 两 段 时 期 意 味 着 典 型 的 周 期 性 衰 退 。']\n",
      "zh vocab_size is :620666\n",
      "finished save zh token\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "with open(\"../translation/data/en-zh/wmt19.en\") as f:\n",
    "    src_en = f.read().splitlines()\n",
    "with open(\"../translation/data/en-zh/wmt19.zh\") as f:\n",
    "    target_zh = f.read().splitlines()\n",
    "# src_en.extend(zimu_src_en)\n",
    "# target_zh.extend(zimu_target_zh)\n",
    "print(src_en[:3])\n",
    "print(target_zh[:3])\n",
    "# 生成英文的token\n",
    "num_words = 2 ** 13\n",
    "oov_token = '<UNK>'\n",
    "tokenizer_en = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "tokenizer_en.fit_on_texts(src_en)\n",
    "VOCAB_SIZE_EN = len(tokenizer_en.word_index)\n",
    "print(\"en vocab_size is :{}\".format(VOCAB_SIZE_EN))\n",
    "token_en_json = tokenizer_en.to_json()\n",
    "\n",
    "with open(\"./token/words_en.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(token_en_json, f, ensure_ascii=False)  # 存为json对象\n",
    "    print(\"finished save en token\")\n",
    "    \n",
    "\n",
    "\n",
    "#生成中文的token\n",
    "num_words = 2 ** 13\n",
    "oov_token = '<UNK>'\n",
    "tokenizer_zh = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "tokenizer_zh.fit_on_texts(target_zh)\n",
    "VOCAB_SIZE_ZH = len(tokenizer_zh.word_index)\n",
    "print(\"zh vocab_size is :{}\".format(VOCAB_SIZE_ZH))\n",
    "token_zh_json = tokenizer_zh.to_json()\n",
    "\n",
    "with open(\"./token/words_zh.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(token_zh_json, f, ensure_ascii=False)  # 存为json对象\n",
    "    print(\"finished save zh token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8acdae03-cc55-4607-b5f5-844a76c708e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_en.word_index['our']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db36250-10dc-4533-8404-3f54b13d1d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=40\n",
    "#英文句子id化\n",
    "START_TOKEN_EN = len(tokenizer_en.word_index) + 1\n",
    "END_TOKENN_EN = len(tokenizer_en.word_index) + 2\n",
    "VOCAB_SIZE_EN = len(tokenizer_en.word_index) + 3\n",
    "tokenized_inputs = tokenizer_en.texts_to_sequences(train_en)\n",
    "#中文句子id化\n",
    "START_TOKEN_ZH = len(tokenizer_zh.word_index) + 1\n",
    "END_TOKENN_ZH = len(tokenizer_zh.word_index) + 2\n",
    "VOCAB_SIZE_ZH = len(tokenizer_zh.word_index) + 3\n",
    "tokenized_outputs = tokenizer_zh.texts_to_sequences(train_zh)\n",
    "\n",
    "# pad token sentences\n",
    "tokenized_inputs = [[START_TOKEN_EN] + i + [END_TOKENN_EN] for i in tokenized_inputs]\n",
    "tokenized_outputs = [[START_TOKEN_ZH] + i + [END_TOKENN_ZH] for i in tokenized_outputs]\n",
    "\n",
    "tokenized_inputs = pad_sequences(tokenized_inputs, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "tokenized_outputs = pad_sequences(tokenized_outputs, maxlen=max_len, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4b4b1f3-5019-40a7-bbca-68db0e724b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24811"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "END_TOKENN_EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3ed46d2-f550-4743-9682-722dc8dadab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class DataProcesser(Dataset):\n",
    "    def __init__(self, first_seg, sencond_seg):\n",
    "        super(DataProcesser, self).__init__()\n",
    "        self.first_seg = first_seg\n",
    "        self.sencond_seg = sencond_seg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sencond_seg)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        seg1 = self.first_seg[item]\n",
    "\n",
    "        seg2 = self.sencond_seg[item]\n",
    "\n",
    "        return (torch.tensor(seg1, dtype=torch.long),\n",
    "                torch.tensor(seg2, dtype=torch.long))\n",
    "\n",
    "dataset = DataProcesser(tokenized_inputs, tokenized_outputs)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d181e45d-df32-4de2-84e0-9cb3f592d887",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AdamWarmup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m transformer \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m adam_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(transformer\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.98\u001b[39m), eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-9\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m transformer_optimizer \u001b[38;5;241m=\u001b[39m AdamWarmup(model_size \u001b[38;5;241m=\u001b[39m d_model, warmup_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4000\u001b[39m, optimizer \u001b[38;5;241m=\u001b[39m adam_optimizer)\n\u001b[1;32m     14\u001b[0m criterion \u001b[38;5;241m=\u001b[39m LossWithLS(VOCAB_SIZE_ZH, \u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit model finished..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AdamWarmup' is not defined"
     ]
    }
   ],
   "source": [
    "d_model = 768\n",
    "heads = 12\n",
    "num_layers = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# epochs = 10\n",
    "\n",
    "\n",
    "    \n",
    "transformer = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, src_vocab_size=VOCAB_SIZE_EN,target_vocab_size=VOCAB_SIZE_ZH)\n",
    "transformer = nn.DataParallel(transformer)\n",
    "transformer = transformer.to(device)\n",
    "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
    "criterion = LossWithLS(VOCAB_SIZE_ZH, 0.1)\n",
    "print(\"init model finished..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "363a35d3-a6ba-45d4-bf9c-08d844abae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, transformer, criterion, epoch):\n",
    "    \n",
    "    transformer.train()\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, (question, reply) in enumerate(train_loader):\n",
    "        \n",
    "        samples = question.shape[0]\n",
    "\n",
    "        # Move to device\n",
    "        question = question.to(device)\n",
    "        reply = reply.to(device)\n",
    "\n",
    "        # Prepare Target Data\n",
    "        reply_input = reply[:, :-1]\n",
    "        reply_target = reply[:, 1:]\n",
    "\n",
    "\n",
    "        # Get the transformer outputs\n",
    "        out = transformer(question, reply_input, True)\n",
    "        # print(out.shape)\n",
    "        # print(out)\n",
    "        # Compute the loss\n",
    "        loss = criterion(out, reply_target)\n",
    "        \n",
    "        # Backprop\n",
    "        transformer_optimizer.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        transformer_optimizer.step()\n",
    "        \n",
    "        sum_loss += loss.item() * samples\n",
    "        count += samples\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))\n",
    "\n",
    "EPOCHS=200\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train(dataloader, transformer, criterion, epoch)\n",
    "    \n",
    "    state = {'epoch': epoch, 'transformer': transformer.module, 'transformer_optimizer': transformer_optimizer}\n",
    "    if ((epoch +1) % 10 == 0 and epoch > 0)  or epoch == EPOCHS - 1:\n",
    "        torch.save(state, './model/model_4layer768_' + str(epoch + 1) + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a87858ed-fd13-487c-8eba-d7223fee7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "import json\n",
    "import torch\n",
    "device='cpu'\n",
    "checkpoint = torch.load('/model_dir/model_6layer768_80.bin',map_location=torch.device('cpu'))\n",
    "model = checkpoint['transformer']\n",
    "\n",
    "with open(\"./token/words_zh.json\") as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer_zh = tokenizer_from_json(data)\n",
    "\n",
    "with open(\"./token/words_en.json\") as f:\n",
    "    data1 = json.load(f)\n",
    "    tokenizers_en = tokenizer_from_json(data1)\n",
    "\n",
    "id2word = tokenizer_zh.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41f657e7-4d61-4b57-8f34-82c38ad274a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'END_TOKENN_ZH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m END_TOKENN_ZH\n",
      "\u001b[0;31mNameError\u001b[0m: name 'END_TOKENN_ZH' is not defined"
     ]
    }
   ],
   "source": [
    "# model\n",
    "END_TOKENN_ZH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20a8aa98-3c17-474f-9abd-715e0fdcd839",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "START_TOKEN_EN = len(tokenizers_en.word_index) + 1\n",
    "END_TOKEN_EN = len(tokenizers_en.word_index) + 2\n",
    "START_TOKEN_ZH = len(tokenizer_zh.word_index) + 1\n",
    "END_TOKEN_ZH= len(tokenizer_zh.word_index) + 2\n",
    "# END_TOKEN_EN=END_TOKENN_EN\n",
    "# END_TOKEN_ZH=END_TOKENN_ZH\n",
    "def evaluate(sentence, model, tokenizer, max_len=64):\n",
    "    model.eval()\n",
    "    sentence = \" \".join(nltk.word_tokenize(sentence.lower()))\n",
    "    print(sentence)\n",
    "  \n",
    "    sentence = [START_TOKEN_EN] + tokenizers_en.texts_to_sequences([sentence])[0] + [END_TOKEN_EN]\n",
    "    \n",
    "    sentence = torch.tensor(sentence, dtype=torch.long).unsqueeze(dim=0).to(device)\n",
    "    # print(sentence)\n",
    "    output = torch.tensor([START_TOKEN_ZH], dtype=torch.long).unsqueeze(dim=0).to(device)\n",
    "    # print(output)\n",
    "    for i in range(max_len):\n",
    "        size = output.shape[1]\n",
    "        predictions = model(sentence,output,training=False)\n",
    "       \n",
    "        predictions = predictions[:, -1:, :]\n",
    "        \n",
    "        pred_id = torch.argmax(predictions, dim=-1)\n",
    "       \n",
    "        if pred_id.unsqueeze(0).item() == END_TOKEN_ZH:\n",
    "            break\n",
    "        output = torch.cat([output, pred_id], dim=-1)\n",
    "        \n",
    "    return output.squeeze(0)\n",
    "\n",
    "def predict(sentence):\n",
    "    predictions = evaluate(sentence, model, tokenizer_zh, max_len=40)\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    # print(predictions)\n",
    "    \n",
    "    predic_senc = [id2word[i] for i in predictions if i <= len(tokenizer_zh.word_index) and i > 0]\n",
    "    return \" \".join(predic_senc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54ec55a-4579-4812-aa64-96c6eb0afa25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predict(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "predict(\"where are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b95215-87b1-49f7-a901-aa39f449a379",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_en' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_en[\u001b[38;5;241m10\u001b[39m:\u001b[38;5;241m20\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_en' is not defined"
     ]
    }
   ],
   "source": [
    "train_en[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8b5eff6-0b16-4cbe-8c6f-109fe65c6a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['上 到 山顶 上 之后',\n",
       " '风 突然 变大 了',\n",
       " '我 在 挪威 美丽 的 西海岸 上',\n",
       " '这里 峡湾 遍布 寒冷刺骨',\n",
       " '还有 身材 健硕 的 北欧 海盗',\n",
       " '我 一直 很 喜欢 这个',\n",
       " '伟大 国家 生产 的 海鲜',\n",
       " '所以 我 在 十二月份 来到 了 这里',\n",
       " '大厨 告诉 我 这个 时候 是 挪威 海鲜',\n",
       " '和 其他 美味佳肴 的 巅峰 时期']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_zh[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4721e0d6-5b44-4488-a111-65cc423dd2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 's okay .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'好 的'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"it's okay.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcc1403d-747a-4bd2-8379-ffda7e94f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [predict(i) for i in test_en[:500]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69d1f099-7565-41a4-9aad-3ed2ebd4df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" -Oh, no, no, no. I don't think   -It's okay, it's okay.\", \" -No, really, I don't need a shower.   -It's it's it's okay.\", \" It's okay.\"]\n",
      "['不 没关系', '不 我 需要 一个 时间', '没关系']\n",
      "['- 哦 不 不 不 我 不 认为 ...- 没关系 没关系', '- 不 真的 我 不 需要 洗澡 -... 它 没关系', '没关系']\n"
     ]
    }
   ],
   "source": [
    "print(test_en[:3])\n",
    "print(pred[:3])\n",
    "print(test_zh[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "588239a1-2b4d-428a-91c1-e7280a20bac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 19:28:01 | INFO | fairseq.file_utils | loading archive file ../translation/model\n",
      "2022-11-10 19:28:02 | INFO | fairseq.tasks.translation | [ja] dictionary: 70912 types\n",
      "2022-11-10 19:28:02 | INFO | fairseq.tasks.translation | [zh] dictionary: 69840 types\n",
      "2022-11-10 19:28:06 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 12, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 8192, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 8192, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 70, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/model_dir', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/DATA/jupyter/personal/translation/data-bin/train.ja-zh', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=4, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=4, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=True, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_gpu=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=70, max_source_positions=1024, max_target_positions=1024, max_tokens=8192, max_tokens_valid=8192, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=4, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/model_dir', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[4], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': '/DATA/jupyter/personal/translation/data-bin/train.ja-zh', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为了 天下@@ 太平 ， 为了 更加 踏实 。\n"
     ]
    }
   ],
   "source": [
    "from fairseq.models.transformer import TransformerModel\n",
    "trans = TransformerModel.from_pretrained(\n",
    "  '../translation/model',\n",
    "  checkpoint_file='checkpoint_best.pt',\n",
    "  data_name_or_path='../data-bin/train.ja-zh',\n",
    "  is_gpu=False\n",
    ")\n",
    "inputs = \"天下 静謐 の ため 　 一層 励む よう に と 。\"\n",
    "print(trans.translate(inputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py3",
   "language": "python",
   "name": "env_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
